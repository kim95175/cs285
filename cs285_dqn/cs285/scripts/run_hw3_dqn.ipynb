{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUl_qfOR8JV6"
   },
   "source": [
    "##Setup\n",
    "\n",
    "You will need to make a copy of this notebook in your Google Drive before you can edit the homework files. You can do so with **File &rarr; Save a copy in Drive**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-colab\n",
      "  Downloading google-colab-1.0.0.tar.gz (72 kB)\n",
      "\u001b[K     |████████████████████████████████| 72 kB 948 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth~=1.4.0\n",
      "  Downloading google_auth-1.4.2-py2.py3-none-any.whl (64 kB)\n",
      "\u001b[K     |████████████████████████████████| 64 kB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting ipykernel~=4.6.0\n",
      "  Downloading ipykernel-4.6.1-py3-none-any.whl (104 kB)\n",
      "\u001b[K     |████████████████████████████████| 104 kB 5.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting ipython~=5.5.0\n",
      "  Downloading ipython-5.5.0-py3-none-any.whl (758 kB)\n",
      "\u001b[K     |████████████████████████████████| 758 kB 6.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting notebook~=5.2.0\n",
      "  Downloading notebook-5.2.2-py2.py3-none-any.whl (8.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 8.0 MB 47.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting six~=1.12.0\n",
      "  Downloading six-1.12.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting pandas~=0.24.0\n",
      "  Downloading pandas-0.24.2-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.1 MB 27.4 MB/s eta 0:00:01eta 0:00:01     |███████████████████████         | 7.2 MB 27.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting portpicker~=1.2.0\n",
      "  Downloading portpicker-1.2.0.tar.gz (17 kB)\n",
      "Collecting requests~=2.21.0\n",
      "  Downloading requests-2.21.0-py2.py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 9.9 MB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting tornado~=4.5.0\n",
      "  Downloading tornado-4.5.3.tar.gz (484 kB)\n",
      "\u001b[K     |████████████████████████████████| 484 kB 101.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cachetools>=2.0.0 in /home/air/anaconda3/envs/elsa/lib/python3.7/site-packages (from google-auth~=1.4.0->google-colab) (4.2.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/air/anaconda3/envs/elsa/lib/python3.7/site-packages (from google-auth~=1.4.0->google-colab) (0.2.8)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /home/air/anaconda3/envs/elsa/lib/python3.7/site-packages (from google-auth~=1.4.0->google-colab) (4.7.2)\n",
      "Requirement already satisfied: jupyter-client in /home/air/anaconda3/envs/elsa/lib/python3.7/site-packages (from ipykernel~=4.6.0->google-colab) (6.1.12)\n",
      "Requirement already satisfied: traitlets>=4.1.0 in /home/air/anaconda3/envs/elsa/lib/python3.7/site-packages (from ipykernel~=4.6.0->google-colab) (5.0.5)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /home/air/anaconda3/envs/elsa/lib/python3.7/site-packages (from ipython~=5.5.0->google-colab) (0.8.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in /home/air/anaconda3/envs/elsa/lib/python3.7/site-packages (from ipython~=5.5.0->google-colab) (51.0.0.post20201207)\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /home/air/anaconda3/envs/elsa/lib/python3.7/site-packages (from ipython~=5.5.0->google-colab) (1.0.18)\n",
      "Requirement already satisfied: pygments in /home/air/anaconda3/envs/elsa/lib/python3.7/site-packages (from ipython~=5.5.0->google-colab) (2.9.0)\n",
      "Requirement already satisfied: pexpect in /home/air/anaconda3/envs/elsa/lib/python3.7/site-packages (from ipython~=5.5.0->google-colab) (4.8.0)\n",
      "Requirement already satisfied: decorator in /home/air/anaconda3/envs/elsa/lib/python3.7/site-packages (from ipython~=5.5.0->google-colab) (4.4.2)\n",
      "Requirement already satisfied: pickleshare in /home/air/anaconda3/envs/elsa/lib/python3.7/site-packages (from ipython~=5.5.0->google-colab) (0.7.5)\n",
      "Requirement already satisfied: jupyter-core in /home/air/anaconda3/envs/elsa/lib/python3.7/site-packages (from notebook~=5.2.0->google-colab) (4.7.1)\n",
      "Requirement already satisfied: ipython-genutils in /home/air/anaconda3/envs/elsa/lib/python3.7/site-packages (from notebook~=5.2.0->google-colab) (0.2.0)\n",
      "Requirement already satisfied: nbformat in /home/air/anaconda3/envs/elsa/lib/python3.7/site-packages (from notebook~=5.2.0->google-colab) (5.1.3)\n",
      "Requirement already satisfied: jinja2 in /home/air/anaconda3/envs/elsa/lib/python3.7/site-packages (from notebook~=5.2.0->google-colab) (3.0.1)\n",
      "Requirement already satisfied: terminado>=0.3.3 in /home/air/anaconda3/envs/elsa/lib/python3.7/site-packages (from notebook~=5.2.0->google-colab) (0.9.4)\n",
      "Requirement already satisfied: nbconvert in /home/air/anaconda3/envs/elsa/lib/python3.7/site-packages (from notebook~=5.2.0->google-colab) (6.1.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/air/anaconda3/envs/elsa/lib/python3.7/site-packages (from pandas~=0.24.0->google-colab) (1.19.2)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /home/air/anaconda3/envs/elsa/lib/python3.7/site-packages (from pandas~=0.24.0->google-colab) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2011k in /home/air/anaconda3/envs/elsa/lib/python3.7/site-packages (from pandas~=0.24.0->google-colab) (2021.1)\n",
      "Requirement already satisfied: wcwidth in /home/air/anaconda3/envs/elsa/lib/python3.7/site-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython~=5.5.0->google-colab) (0.2.5)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/air/anaconda3/envs/elsa/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth~=1.4.0->google-colab) (0.4.8)\n",
      "Collecting urllib3<1.25,>=1.21.1\n",
      "  Downloading urllib3-1.24.3-py2.py3-none-any.whl (118 kB)\n",
      "\u001b[K     |████████████████████████████████| 118 kB 105.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /home/air/anaconda3/envs/elsa/lib/python3.7/site-packages (from requests~=2.21.0->google-colab) (2021.5.30)\n",
      "Collecting chardet<3.1.0,>=3.0.2\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 142.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna<2.9,>=2.5\n",
      "  Downloading idna-2.8-py2.py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 11.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: ptyprocess in /home/air/anaconda3/envs/elsa/lib/python3.7/site-packages (from terminado>=0.3.3->notebook~=5.2.0->google-colab) (0.7.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/air/anaconda3/envs/elsa/lib/python3.7/site-packages (from jinja2->notebook~=5.2.0->google-colab) (2.0.1)\n",
      "Requirement already satisfied: pyzmq>=13 in /home/air/anaconda3/envs/elsa/lib/python3.7/site-packages (from jupyter-client->ipykernel~=4.6.0->google-colab) (20.0.0)\n",
      "Requirement already satisfied: bleach in /home/air/anaconda3/envs/elsa/lib/python3.7/site-packages (from nbconvert->notebook~=5.2.0->google-colab) (3.3.0)\n",
      "Requirement already satisfied: testpath in /home/air/anaconda3/envs/elsa/lib/python3.7/site-packages (from nbconvert->notebook~=5.2.0->google-colab) (0.5.0)\n",
      "Requirement already satisfied: defusedxml in /home/air/anaconda3/envs/elsa/lib/python3.7/site-packages (from nbconvert->notebook~=5.2.0->google-colab) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /home/air/anaconda3/envs/elsa/lib/python3.7/site-packages (from nbconvert->notebook~=5.2.0->google-colab) (0.1.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /home/air/anaconda3/envs/elsa/lib/python3.7/site-packages (from nbconvert->notebook~=5.2.0->google-colab) (1.4.3)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /home/air/anaconda3/envs/elsa/lib/python3.7/site-packages (from nbconvert->notebook~=5.2.0->google-colab) (0.3)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /home/air/anaconda3/envs/elsa/lib/python3.7/site-packages (from nbconvert->notebook~=5.2.0->google-colab) (0.5.3)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /home/air/anaconda3/envs/elsa/lib/python3.7/site-packages (from nbconvert->notebook~=5.2.0->google-colab) (0.8.4)\n",
      "Requirement already satisfied: async-generator in /home/air/anaconda3/envs/elsa/lib/python3.7/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook~=5.2.0->google-colab) (1.10)\n",
      "Requirement already satisfied: nest-asyncio in /home/air/anaconda3/envs/elsa/lib/python3.7/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook~=5.2.0->google-colab) (1.5.1)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /home/air/anaconda3/envs/elsa/lib/python3.7/site-packages (from nbformat->notebook~=5.2.0->google-colab) (3.2.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/air/anaconda3/envs/elsa/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook~=5.2.0->google-colab) (3.10.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /home/air/anaconda3/envs/elsa/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook~=5.2.0->google-colab) (0.17.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /home/air/anaconda3/envs/elsa/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook~=5.2.0->google-colab) (21.2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: webencodings in /home/air/anaconda3/envs/elsa/lib/python3.7/site-packages (from bleach->nbconvert->notebook~=5.2.0->google-colab) (0.5.1)\n",
      "Requirement already satisfied: packaging in /home/air/anaconda3/envs/elsa/lib/python3.7/site-packages (from bleach->nbconvert->notebook~=5.2.0->google-colab) (21.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/air/anaconda3/envs/elsa/lib/python3.7/site-packages (from importlib-metadata->jsonschema!=2.5.0,>=2.4->nbformat->notebook~=5.2.0->google-colab) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/air/anaconda3/envs/elsa/lib/python3.7/site-packages (from importlib-metadata->jsonschema!=2.5.0,>=2.4->nbformat->notebook~=5.2.0->google-colab) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/air/anaconda3/envs/elsa/lib/python3.7/site-packages (from packaging->bleach->nbconvert->notebook~=5.2.0->google-colab) (2.4.7)\n",
      "Building wheels for collected packages: google-colab, portpicker, tornado\n",
      "  Building wheel for google-colab (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for google-colab: filename=google_colab-1.0.0-py2.py3-none-any.whl size=102289 sha256=2b7ea081cbf7508fa494c0958c0c91c98743f4a92fa81473cb523643cda2c5c3\n",
      "  Stored in directory: /home/air/.cache/pip/wheels/f6/3b/58/f34ea9045a7c69bd5634978bf25ac60277e90997d9e6e74192\n",
      "  Building wheel for portpicker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for portpicker: filename=portpicker-1.2.0-py3-none-any.whl size=13371 sha256=bfe4f1403c1cb4d05a7fcfcd36526c1a30b6e21d198228c2f66fc81f19746294\n",
      "  Stored in directory: /home/air/.cache/pip/wheels/73/0c/f5/35977446e45e818e6b848be3d41e7f38298a5102f4dcda21c6\n",
      "  Building wheel for tornado (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tornado: filename=tornado-4.5.3-cp37-cp37m-linux_x86_64.whl size=434143 sha256=e8d0f2812401dd9c13d452e6571b753e038e5750ba33222be2203929e152889b\n",
      "  Stored in directory: /home/air/.cache/pip/wheels/a2/45/43/36ec7a893e16c1212a6b1505ded0a2d73cf8e863a0227c8e04\n",
      "Successfully built google-colab portpicker tornado\n",
      "Installing collected packages: six, tornado, ipython, urllib3, ipykernel, idna, chardet, requests, portpicker, pandas, notebook, google-auth, google-colab\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.15.0\n",
      "    Uninstalling six-1.15.0:\n",
      "      Successfully uninstalled six-1.15.0\n",
      "  Attempting uninstall: tornado\n",
      "    Found existing installation: tornado 6.1\n",
      "    Uninstalling tornado-6.1:\n",
      "      Successfully uninstalled tornado-6.1\n",
      "  Attempting uninstall: ipython\n",
      "    Found existing installation: ipython 6.4.0\n",
      "    Uninstalling ipython-6.4.0:\n",
      "      Successfully uninstalled ipython-6.4.0\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.3\n",
      "    Uninstalling urllib3-1.26.3:\n",
      "      Successfully uninstalled urllib3-1.26.3\n",
      "  Attempting uninstall: ipykernel\n",
      "    Found existing installation: ipykernel 5.3.4\n",
      "    Uninstalling ipykernel-5.3.4:\n",
      "      Successfully uninstalled ipykernel-5.3.4\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 2.10\n",
      "    Uninstalling idna-2.10:\n",
      "      Successfully uninstalled idna-2.10\n",
      "  Attempting uninstall: chardet\n",
      "    Found existing installation: chardet 4.0.0\n",
      "    Uninstalling chardet-4.0.0:\n",
      "      Successfully uninstalled chardet-4.0.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.25.1\n",
      "    Uninstalling requests-2.25.1:\n",
      "      Successfully uninstalled requests-2.25.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.2.3\n",
      "    Uninstalling pandas-1.2.3:\n",
      "      Successfully uninstalled pandas-1.2.3\n",
      "  Attempting uninstall: notebook\n",
      "    Found existing installation: notebook 6.4.0\n",
      "    Uninstalling notebook-6.4.0:\n",
      "      Successfully uninstalled notebook-6.4.0\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 1.27.1\n",
      "    Uninstalling google-auth-1.27.1:\n",
      "      Successfully uninstalled google-auth-1.27.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorboard 2.3.0 requires google-auth<2,>=1.6.3, but you have google-auth 1.4.2 which is incompatible.\u001b[0m\n",
      "Successfully installed chardet-3.0.4 google-auth-1.4.2 google-colab-1.0.0 idna-2.8 ipykernel-4.6.1 ipython-5.5.0 notebook-5.2.2 pandas-0.24.2 portpicker-1.2.0 requests-2.21.0 six-1.12.0 tornado-4.5.3 urllib3-1.24.3\n"
     ]
    }
   ],
   "source": [
    "!pip install google-colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /home/air/anaconda3/envs/elsa:\r\n",
      "#\r\n",
      "# Name                    Version                   Build  Channel\r\n",
      "_libgcc_mutex             0.1                        main  \r\n",
      "absl-py                   0.12.0                   pypi_0    pypi\r\n",
      "argon2-cffi               20.1.0           py37h27cfd23_1  \r\n",
      "async_generator           1.10             py37h28b3542_0  \r\n",
      "atari-py                  0.2.6                    pypi_0    pypi\r\n",
      "attrs                     20.3.0                   pypi_0    pypi\r\n",
      "automat                   20.2.0                   pypi_0    pypi\r\n",
      "backcall                  0.2.0              pyhd3eb1b0_0  \r\n",
      "blas                      1.0                         mkl  \r\n",
      "bleach                    3.3.0              pyhd3eb1b0_0  \r\n",
      "box2d-py                  2.3.8                    pypi_0    pypi\r\n",
      "ca-certificates           2021.7.5             h06a4308_1  \r\n",
      "cachetools                4.2.1                    pypi_0    pypi\r\n",
      "certifi                   2021.5.30        py37h06a4308_0  \r\n",
      "cffi                      1.14.5           py37h261ae71_0  \r\n",
      "chardet                   3.0.4                    pypi_0    pypi\r\n",
      "cloudpickle               1.3.0                    pypi_0    pypi\r\n",
      "constantly                15.1.0                   pypi_0    pypi\r\n",
      "cryptography              3.4.6                    pypi_0    pypi\r\n",
      "cs285                     0.1.0                     dev_0    <develop>\r\n",
      "cudatoolkit               11.0.221             h6bb024c_0  \r\n",
      "cycler                    0.10.0                   pypi_0    pypi\r\n",
      "cython                    0.29.22                  pypi_0    pypi\r\n",
      "decorator                 4.4.2                    pypi_0    pypi\r\n",
      "defusedxml                0.7.1              pyhd3eb1b0_0  \r\n",
      "docker-pycreds            0.2.1                    pypi_0    pypi\r\n",
      "easyprocess               0.3                      pypi_0    pypi\r\n",
      "entrypoints               0.3                      py37_0  \r\n",
      "filelock                  3.0.12                   pypi_0    pypi\r\n",
      "freetype                  2.10.4               h5ab3b9f_0  \r\n",
      "future                    0.18.2                   pypi_0    pypi\r\n",
      "glcontext                 2.3.2                    pypi_0    pypi\r\n",
      "glfw                      1.12.0                   pypi_0    pypi\r\n",
      "google-auth               1.4.2                    pypi_0    pypi\r\n",
      "google-auth-oauthlib      0.4.3                    pypi_0    pypi\r\n",
      "google-colab              1.0.0                    pypi_0    pypi\r\n",
      "grpcio                    1.36.1                   pypi_0    pypi\r\n",
      "gym                       0.17.2                   pypi_0    pypi\r\n",
      "gym3                      0.3.3                    pypi_0    pypi\r\n",
      "hyperlink                 21.0.0                   pypi_0    pypi\r\n",
      "idna                      2.8                      pypi_0    pypi\r\n",
      "imageio                   2.9.0                    pypi_0    pypi\r\n",
      "imageio-ffmpeg            0.3.0                    pypi_0    pypi\r\n",
      "importlib-metadata        3.7.3                    pypi_0    pypi\r\n",
      "importlib_metadata        3.10.0               hd3eb1b0_0  \r\n",
      "incremental               17.5.0                   pypi_0    pypi\r\n",
      "intel-openmp              2020.2                      254  \r\n",
      "ipdb                      0.13.3                   pypi_0    pypi\r\n",
      "ipykernel                 4.6.1                    pypi_0    pypi\r\n",
      "ipython                   5.5.0                    pypi_0    pypi\r\n",
      "ipython_genutils          0.2.0              pyhd3eb1b0_1  \r\n",
      "jedi                      0.18.0                   pypi_0    pypi\r\n",
      "jinja2                    3.0.1              pyhd3eb1b0_0  \r\n",
      "jpeg                      9b                   h024ee3a_2  \r\n",
      "jsonschema                3.2.0                      py_2  \r\n",
      "jupyter_client            6.1.12             pyhd3eb1b0_0  \r\n",
      "jupyter_core              4.7.1            py37h06a4308_0  \r\n",
      "jupyterlab_pygments       0.1.2                      py_0  \r\n",
      "kiwisolver                1.3.1                    pypi_0    pypi\r\n",
      "kornia                    0.4.1                    pypi_0    pypi\r\n",
      "lcms2                     2.11                 h396b838_0  \r\n",
      "ld_impl_linux-64          2.33.1               h53a641e_7  \r\n",
      "libedit                   3.1.20191231         h14c3975_1  \r\n",
      "libffi                    3.3                  he6710b0_2  \r\n",
      "libgcc-ng                 9.1.0                hdf63c60_0  \r\n",
      "libpng                    1.6.37               hbc83047_0  \r\n",
      "libsodium                 1.0.18               h7b6447c_0  \r\n",
      "libstdcxx-ng              9.1.0                hdf63c60_0  \r\n",
      "libtiff                   4.1.0                h2733197_1  \r\n",
      "libuv                     1.40.0               h7b6447c_0  \r\n",
      "lockfile                  0.12.2                   pypi_0    pypi\r\n",
      "lz4-c                     1.9.2                heb0550a_3  \r\n",
      "markdown                  3.3.4                    pypi_0    pypi\r\n",
      "markupsafe                2.0.1            py37h27cfd23_0  \r\n",
      "matplotlib                2.2.2                    pypi_0    pypi\r\n",
      "mistune                   0.8.4           py37h14c3975_1001  \r\n",
      "mkl                       2020.2                      256  \r\n",
      "mkl-service               2.3.0            py37he8ac12f_0  \r\n",
      "mkl_fft                   1.2.0            py37h23d657b_0  \r\n",
      "mkl_random                1.1.1            py37h0573a6f_0  \r\n",
      "moderngl                  5.6.4                    pypi_0    pypi\r\n",
      "moviepy                   1.0.0                    pypi_0    pypi\r\n",
      "mujoco-py                 1.50.1.56                pypi_0    pypi\r\n",
      "nb_conda_kernels          2.3.1            py37h06a4308_0  \r\n",
      "nbclient                  0.5.3              pyhd3eb1b0_0  \r\n",
      "nbconvert                 6.1.0            py37h06a4308_0  \r\n",
      "nbformat                  5.1.3              pyhd3eb1b0_0  \r\n",
      "ncurses                   6.2                  he6710b0_1  \r\n",
      "nest-asyncio              1.5.1              pyhd3eb1b0_0  \r\n",
      "networkx                  2.5                      pypi_0    pypi\r\n",
      "ninja                     1.10.2           py37hff7bd54_0  \r\n",
      "notebook                  5.2.2                    pypi_0    pypi\r\n",
      "numpy                     1.19.2           py37h54aff64_0  \r\n",
      "numpy-base                1.19.2           py37hfa32c7d_0  \r\n",
      "oauthlib                  3.1.0                    pypi_0    pypi\r\n",
      "olefile                   0.46                     py37_0  \r\n",
      "opencv-python             4.4.0.42                 pypi_0    pypi\r\n",
      "openssl                   1.1.1k               h27cfd23_0  \r\n",
      "packaging                 21.0               pyhd3eb1b0_0  \r\n",
      "pandas                    0.24.2                   pypi_0    pypi\r\n",
      "pandocfilters             1.4.3            py37h06a4308_1  \r\n",
      "parso                     0.8.1                    pypi_0    pypi\r\n",
      "pexpect                   4.8.0              pyhd3eb1b0_3  \r\n",
      "pickleshare               0.7.5           pyhd3eb1b0_1003  \r\n",
      "pillow                    7.2.0                    pypi_0    pypi\r\n",
      "pip                       21.0.1                   pypi_0    pypi\r\n",
      "plotly                    4.14.1                   pypi_0    pypi\r\n",
      "portpicker                1.2.0                    pypi_0    pypi\r\n",
      "procgen                   0.10.4                   pypi_0    pypi\r\n",
      "proglog                   0.1.9                    pypi_0    pypi\r\n",
      "prometheus_client         0.11.0             pyhd3eb1b0_0  \r\n",
      "prompt-toolkit            1.0.18                   pypi_0    pypi\r\n",
      "protobuf                  3.15.6                   pypi_0    pypi\r\n",
      "ptyprocess                0.7.0              pyhd3eb1b0_2  \r\n",
      "pyasn1                    0.4.8                    pypi_0    pypi\r\n",
      "pyasn1-modules            0.2.8                    pypi_0    pypi\r\n",
      "pycparser                 2.20                       py_2  \r\n",
      "pyglet                    1.5.0                    pypi_0    pypi\r\n",
      "pygments                  2.8.1                    pypi_0    pypi\r\n",
      "pyhamcrest                2.0.2                    pypi_0    pypi\r\n",
      "pyparsing                 2.4.7              pyhd3eb1b0_0  \r\n",
      "pyrsistent                0.17.3           py37h7b6447c_0  \r\n",
      "python                    3.7.9                h7579374_0  \r\n",
      "python-dateutil           2.8.1              pyhd3eb1b0_0  \r\n",
      "pytz                      2021.1                   pypi_0    pypi\r\n",
      "pyvirtualdisplay          1.3.2                    pypi_0    pypi\r\n",
      "pyyaml                    5.4.1                    pypi_0    pypi\r\n",
      "pyzmq                     20.0.0           py37h2531618_1  \r\n",
      "readline                  8.0                  h7b6447c_0  \r\n",
      "requests                  2.21.0                   pypi_0    pypi\r\n",
      "requests-oauthlib         1.3.0                    pypi_0    pypi\r\n",
      "retrying                  1.3.3                    pypi_0    pypi\r\n",
      "rsa                       4.7.2                    pypi_0    pypi\r\n",
      "scipy                     1.6.1                    pypi_0    pypi\r\n",
      "seaborn                   0.11.1                   pypi_0    pypi\r\n",
      "send2trash                1.5.0              pyhd3eb1b0_1  \r\n",
      "setuptools                51.0.0           py37h06a4308_2  \r\n",
      "simplegeneric             0.8.1                    pypi_0    pypi\r\n",
      "six                       1.12.0                   pypi_0    pypi\r\n",
      "sqlite                    3.33.0               h62c20be_0  \r\n",
      "tensorboard               2.3.0                    pypi_0    pypi\r\n",
      "tensorboard-plugin-wit    1.8.0                    pypi_0    pypi\r\n",
      "tensorboardx              1.8                      pypi_0    pypi\r\n",
      "terminado                 0.9.4            py37h06a4308_0  \r\n",
      "testpath                  0.5.0              pyhd3eb1b0_0  \r\n",
      "tk                        8.6.10               hbc83047_0  \r\n",
      "torch                     1.5.1                    pypi_0    pypi\r\n",
      "torchvision               0.8.2                    pypi_0    pypi\r\n",
      "tornado                   4.5.3                    pypi_0    pypi\r\n",
      "tqdm                      4.54.1                   pypi_0    pypi\r\n",
      "traitlets                 5.0.5              pyhd3eb1b0_0  \r\n",
      "twisted                   20.3.0                   pypi_0    pypi\r\n",
      "txaio                     21.2.1                   pypi_0    pypi\r\n",
      "typing_extensions         3.7.4.3                    py_0  \r\n",
      "ujson                     4.0.2                    pypi_0    pypi\r\n",
      "urllib3                   1.24.3                   pypi_0    pypi\r\n",
      "wcwidth                   0.2.5                      py_0  \r\n",
      "webencodings              0.5.1                    py37_1  \r\n",
      "websocket-client          0.57.0                   pypi_0    pypi\r\n",
      "werkzeug                  1.0.1                    pypi_0    pypi\r\n",
      "wheel                     0.36.2             pyhd3eb1b0_0  \r\n",
      "xz                        5.2.5                h7b6447c_0  \r\n",
      "zeromq                    4.3.4                h2531618_0  \r\n",
      "zipp                      3.4.1                    pypi_0    pypi\r\n",
      "zlib                      1.2.11               h7b6447c_3  \r\n",
      "zope-interface            5.2.0                    pypi_0    pypi\r\n",
      "zstd                      1.4.5                h9ceee32_0  \r\n"
     ]
    }
   ],
   "source": [
    "!conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellView": "form",
    "id": "g5xIOIpW8_jC"
   },
   "outputs": [
    {
     "ename": "EasyProcessError",
     "evalue": "start error <EasyProcess cmd_param=['Xvfb', '-help'] cmd=['Xvfb', '-help'] oserror=[Errno 2] No such file or directory: 'Xvfb': 'Xvfb' return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/elsa/lib/python3.7/site-packages/easyprocess/__init__.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    168\u001b[0m             self.popen = subprocess.Popen(\n\u001b[0;32m--> 169\u001b[0;31m                 \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m             )\n",
      "\u001b[0;32m~/anaconda3/envs/elsa/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    799\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    801\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/elsa/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1550\u001b[0m                             \u001b[0merr_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1551\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Xvfb': 'Xvfb'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mEasyProcessError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-40b8bc6e518e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyvirtualdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdisplay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisible\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m900\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/elsa/lib/python3.7/site-packages/pyvirtualdisplay/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, backend, visible, size, color_depth, bgcolor, use_xauth, randomizer, retries, extra_args, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;31m# check_startup=check_startup,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mextra_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextra_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         )\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/elsa/lib/python3.7/site-packages/pyvirtualdisplay/xvfb.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, size, color_depth, bgcolor, use_xauth, fbdir, dpi, randomizer, retries, extra_args)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mrandomizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandomizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mextra_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextra_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         )\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/elsa/lib/python3.7/site-packages/pyvirtualdisplay/abstractdisplay.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, program, use_xauth, randomizer, retries, extra_args)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_started\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mhelptext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_helptext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprogram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_displayfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"-displayfd\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhelptext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_displayfd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/elsa/lib/python3.7/site-packages/pyvirtualdisplay/util.py\u001b[0m in \u001b[0;36mget_helptext\u001b[0;34m(program)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_stdout_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_stderr_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mhelptext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhelptext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/elsa/lib/python3.7/site-packages/easyprocess/__init__.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \"\"\"\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/elsa/lib/python3.7/site-packages/easyprocess/__init__.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OSError exception: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moserror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moserror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moserror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEasyProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"start error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_started\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"process was started (pid=%s)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEasyProcessError\u001b[0m: start error <EasyProcess cmd_param=['Xvfb', '-help'] cmd=['Xvfb', '-help'] oserror=[Errno 2] No such file or directory: 'Xvfb': 'Xvfb' return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
     ]
    }
   ],
   "source": [
    "#@title set up virtual display\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "# For later\n",
    "from cs285.infrastructure.colab_utils import (\n",
    "    wrap_env,\n",
    "    show_video\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QizpiHDh9Fwk"
   },
   "source": [
    "## Editing Code\n",
    "\n",
    "To edit code, click the folder icon on the left menu. Navigate to the corresponding file (`cs285_f2020/...`). Double click a file to open an editor. There is a timeout of about ~12 hours with Colab while it is active (and less if you close your browser window). We sync your edits to Google Drive so that you won't lose your work in the event of an instance timeout, but you will need to re-mount your Google Drive and re-install packages with every new instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nii6qk2C9Ipk"
   },
   "source": [
    "## Run DQN and Double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "4t7FUeEG9Dkf"
   },
   "outputs": [],
   "source": [
    "#@title imports\n",
    "import os\n",
    "import time\n",
    "\n",
    "from cs285.infrastructure.rl_trainer import RL_Trainer\n",
    "from cs285.agents.dqn_agent import DQNAgent\n",
    "from cs285.infrastructure.dqn_utils import get_env_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellView": "both",
    "id": "2fXlzARJ9i-t"
   },
   "outputs": [],
   "source": [
    "#@title runtime arguments\n",
    "\n",
    "class Args:\n",
    "\n",
    "  def __getitem__(self, key):\n",
    "    return getattr(self, key)\n",
    "\n",
    "  def __setitem__(self, key, val):\n",
    "    setattr(self, key, val)\n",
    "\n",
    "  def __contains__(self, key):\n",
    "    return hasattr(self, key)\n",
    "\n",
    "  env_name = 'LunarLander-v3' #@param ['MsPacman-v0', 'LunarLander-v3', 'PongNoFrameSkip-v4']\n",
    "  exp_name = 'q3_dqn' #@param\n",
    "\n",
    "  ## PDF will tell you how to set ep_len\n",
    "  ## and discount for each environment\n",
    "  ep_len = 200 #@param {type: \"integer\"}\n",
    "\n",
    "  #@markdown batches and steps\n",
    "  batch_size = 32 #@param {type: \"integer\"}\n",
    "  eval_batch_size = 1000 #@param {type: \"integer\"}\n",
    "\n",
    "  num_agent_train_steps_per_iter = 1 #@param {type: \"integer\"}\n",
    "\n",
    "  num_critic_updates_per_agent_update = 1 #@param {type: \"integer\"}\n",
    "  \n",
    "  #@markdown Q-learning parameters\n",
    "  double_q = False #@param {type: \"boolean\"}\n",
    "\n",
    "  #@markdown system\n",
    "  save_params = False #@param {type: \"boolean\"}\n",
    "  no_gpu = False #@param {type: \"boolean\"}\n",
    "  which_gpu = 0 #@param {type: \"integer\"}\n",
    "  seed = 1 #@param {type: \"integer\"}\n",
    "\n",
    "  #@markdown logging\n",
    "  ## default is to not log video so\n",
    "  ## that logs are small enough to be\n",
    "  ## uploaded to gradscope\n",
    "  video_log_freq =  -1 #@param {type: \"integer\"}\n",
    "  scalar_log_freq =  10000#@param {type: \"integer\"}\n",
    "\n",
    "\n",
    "args = Args()\n",
    "\n",
    "## ensure compatibility with hw1 code\n",
    "args['train_batch_size'] = args['batch_size']\n",
    "\n",
    "if args['video_log_freq'] > 0:\n",
    "  import warnings\n",
    "  warnings.warn(\n",
    "      '''\\nLogging videos will make eventfiles too'''\n",
    "      '''\\nlarge for the autograder. Set video_log_freq = -1'''\n",
    "      '''\\nfor the runs you intend to submit.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "T0cJlp6s-ogO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGGING TO:  /home/air/workspace/cs285/cs285_dqn/cs285/data/q3_dqn_LunarLander-v3_08-07-2021_16-16-03\n"
     ]
    }
   ],
   "source": [
    "#@title create directories for logging\n",
    "\n",
    "#data_path = '''/content/cs285_f2020/''' \\\n",
    "#        '''homework_fall2020/hw3/data'''\n",
    "data_path = os.path.join(os.path.dirname(os.path.abspath('')), 'data')\n",
    "\n",
    "if not (os.path.exists(data_path)):\n",
    "    os.makedirs(data_path)\n",
    "\n",
    "logdir = args.exp_name + '_' + args.env_name + '_' + time.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "logdir = os.path.join(data_path, logdir)\n",
    "args['logdir'] = logdir\n",
    "if not(os.path.exists(logdir)):\n",
    "    os.makedirs(logdir)\n",
    "\n",
    "print(\"LOGGING TO: \", logdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "I525KFRN-42s"
   },
   "outputs": [],
   "source": [
    "#@title Define Q-function trainer\n",
    "\n",
    "class Q_Trainer(object):\n",
    "\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "\n",
    "        train_args = {\n",
    "            'num_agent_train_steps_per_iter': params['num_agent_train_steps_per_iter'],\n",
    "            'num_critic_updates_per_agent_update': params['num_critic_updates_per_agent_update'],\n",
    "            'train_batch_size': params['batch_size'],\n",
    "            'double_q': params['double_q'],\n",
    "        }\n",
    "\n",
    "        env_args = get_env_kwargs(params['env_name'])\n",
    "\n",
    "        for k, v in env_args.items():\n",
    "          params[k] = v\n",
    "\n",
    "        self.params['agent_class'] = DQNAgent\n",
    "        self.params['agent_params'] = params\n",
    "        self.params['train_batch_size'] = params['batch_size']\n",
    "        self.params['env_wrappers'] = env_args['env_wrappers']\n",
    "\n",
    "        self.rl_trainer = RL_Trainer(self.params)\n",
    "\n",
    "    def run_training_loop(self):\n",
    "        self.rl_trainer.run_training_loop(\n",
    "            self.params['num_timesteps'],\n",
    "            collect_policy = self.rl_trainer.agent.actor,\n",
    "            eval_policy = self.rl_trainer.agent.actor,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "wF4LSRGn-_Cv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################\n",
      "logging outputs to  /home/air/workspace/cs285/cs285_dqn/cs285/data/q3_dqn_LunarLander-v3_08-07-2021_16-16-03\n",
      "########################\n",
      "Using GPU id 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/air/anaconda3/envs/elsa/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/air/anaconda3/envs/elsa/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Environment '<class 'cs285.envs.box2d.lunar_lander.LunarLander'>' has deprecated methods '_step' and '_reset' rather than 'step' and 'reset'. Compatibility code invoked. Set _gym_disable_underscore_compat = True to disable this behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1\n",
      "mean reward (100 episodes) nan\n",
      "best mean reward -inf\n",
      "running time 0.006235\n",
      "Train_EnvstepsSoFar : 1\n",
      "TimeSinceStart : 0.006235361099243164\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10001\n",
      "mean reward (100 episodes) -226.384969\n",
      "best mean reward -inf\n",
      "running time 26.892555\n",
      "Train_EnvstepsSoFar : 10001\n",
      "Train_AverageReturn : -226.3849686092798\n",
      "TimeSinceStart : 26.892554998397827\n",
      "Training Loss : 0.23619264364242554\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 11000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 12000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 13000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 14000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 15000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 16000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 17000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 18000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 19000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 20000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 20001\n",
      "mean reward (100 episodes) -183.932324\n",
      "best mean reward -183.932324\n",
      "running time 52.331327\n",
      "Train_EnvstepsSoFar : 20001\n",
      "Train_AverageReturn : -183.93232413167246\n",
      "Train_BestReturn : -183.93232413167246\n",
      "TimeSinceStart : 52.331326723098755\n",
      "Training Loss : 0.5678141713142395\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 21000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 22000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 23000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 24000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 25000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 26000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 27000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 28000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 29000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 30000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 30001\n",
      "mean reward (100 episodes) -165.132492\n",
      "best mean reward -165.132492\n",
      "running time 95.435529\n",
      "Train_EnvstepsSoFar : 30001\n",
      "Train_AverageReturn : -165.1324921851225\n",
      "Train_BestReturn : -165.1324921851225\n",
      "TimeSinceStart : 95.43552851676941\n",
      "Training Loss : 0.45247936248779297\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 31000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 32000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 33000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 34000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 35000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 36000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 37000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 38000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 39000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 40000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 40001\n",
      "mean reward (100 episodes) -161.452578\n",
      "best mean reward -161.452578\n",
      "running time 128.974009\n",
      "Train_EnvstepsSoFar : 40001\n",
      "Train_AverageReturn : -161.4525781988509\n",
      "Train_BestReturn : -161.4525781988509\n",
      "TimeSinceStart : 128.9740092754364\n",
      "Training Loss : 0.18255876004695892\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 41000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 42000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 43000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 44000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 45000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 46000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 47000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 48000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 49000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 50000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 50001\n",
      "mean reward (100 episodes) -152.508857\n",
      "best mean reward -152.508857\n",
      "running time 161.927780\n",
      "Train_EnvstepsSoFar : 50001\n",
      "Train_AverageReturn : -152.5088565895155\n",
      "Train_BestReturn : -152.5088565895155\n",
      "TimeSinceStart : 161.9277799129486\n",
      "Training Loss : 1.6465827226638794\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 51000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 52000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 53000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 54000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 55000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 56000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 57000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 58000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 59000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 60000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 60001\n",
      "mean reward (100 episodes) -147.134303\n",
      "best mean reward -147.134303\n",
      "running time 194.629586\n",
      "Train_EnvstepsSoFar : 60001\n",
      "Train_AverageReturn : -147.1343032782115\n",
      "Train_BestReturn : -147.1343032782115\n",
      "TimeSinceStart : 194.62958645820618\n",
      "Training Loss : 0.9594999551773071\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 61000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 62000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 63000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 64000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 65000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 66000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 67000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 68000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 69000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 70000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 70001\n",
      "mean reward (100 episodes) -124.446295\n",
      "best mean reward -124.446295\n",
      "running time 224.953353\n",
      "Train_EnvstepsSoFar : 70001\n",
      "Train_AverageReturn : -124.44629475561446\n",
      "Train_BestReturn : -124.44629475561446\n",
      "TimeSinceStart : 224.95335268974304\n",
      "Training Loss : 0.29132476449012756\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 71000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 72000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 73000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 74000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 75000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 76000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 77000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 78000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 79000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 80000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 80001\n",
      "mean reward (100 episodes) -84.819709\n",
      "best mean reward -84.819709\n",
      "running time 253.511394\n",
      "Train_EnvstepsSoFar : 80001\n",
      "Train_AverageReturn : -84.81970869100192\n",
      "Train_BestReturn : -84.81970869100192\n",
      "TimeSinceStart : 253.5113935470581\n",
      "Training Loss : 0.3079681992530823\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 81000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 82000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 83000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 84000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 85000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 86000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 87000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 88000 ************\n",
      "\n",
      "Training agent...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 89000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 90000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 90001\n",
      "mean reward (100 episodes) -59.174972\n",
      "best mean reward -59.174972\n",
      "running time 282.427588\n",
      "Train_EnvstepsSoFar : 90001\n",
      "Train_AverageReturn : -59.174971756498906\n",
      "Train_BestReturn : -59.174971756498906\n",
      "TimeSinceStart : 282.4275875091553\n",
      "Training Loss : 0.11266826093196869\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 91000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 92000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 93000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 94000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 95000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 96000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 97000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 98000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 99000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 100000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 100001\n",
      "mean reward (100 episodes) -24.035003\n",
      "best mean reward -24.035003\n",
      "running time 319.310027\n",
      "Train_EnvstepsSoFar : 100001\n",
      "Train_AverageReturn : -24.03500300560221\n",
      "Train_BestReturn : -24.03500300560221\n",
      "TimeSinceStart : 319.31002736091614\n",
      "Training Loss : 2.1472601890563965\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 101000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 102000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 103000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 104000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 105000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 106000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 107000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 108000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 109000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 110000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 110001\n",
      "mean reward (100 episodes) 8.874553\n",
      "best mean reward 8.874553\n",
      "running time 350.586658\n",
      "Train_EnvstepsSoFar : 110001\n",
      "Train_AverageReturn : 8.874552892493945\n",
      "Train_BestReturn : 8.874552892493945\n",
      "TimeSinceStart : 350.58665776252747\n",
      "Training Loss : 0.2063027173280716\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 111000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 112000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 113000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 114000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 115000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 116000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 117000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 118000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 119000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 120000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 120001\n",
      "mean reward (100 episodes) 35.285728\n",
      "best mean reward 35.285728\n",
      "running time 380.770472\n",
      "Train_EnvstepsSoFar : 120001\n",
      "Train_AverageReturn : 35.28572813364236\n",
      "Train_BestReturn : 35.28572813364236\n",
      "TimeSinceStart : 380.77047204971313\n",
      "Training Loss : 1.9809491634368896\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 121000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 122000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 123000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 124000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 125000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 126000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 127000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 128000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 129000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 130000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 130001\n",
      "mean reward (100 episodes) 56.249959\n",
      "best mean reward 56.249959\n",
      "running time 412.279962\n",
      "Train_EnvstepsSoFar : 130001\n",
      "Train_AverageReturn : 56.24995859457739\n",
      "Train_BestReturn : 56.24995859457739\n",
      "TimeSinceStart : 412.2799618244171\n",
      "Training Loss : 0.16850200295448303\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 131000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 132000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 133000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 134000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 135000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 136000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 137000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 138000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 139000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 140000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 140001\n",
      "mean reward (100 episodes) 52.861387\n",
      "best mean reward 56.249959\n",
      "running time 443.103362\n",
      "Train_EnvstepsSoFar : 140001\n",
      "Train_AverageReturn : 52.86138699052808\n",
      "Train_BestReturn : 56.24995859457739\n",
      "TimeSinceStart : 443.1033618450165\n",
      "Training Loss : 0.12003369629383087\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 141000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 142000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 143000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 144000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 145000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 146000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 147000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 148000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 149000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 150000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 150001\n",
      "mean reward (100 episodes) 51.538693\n",
      "best mean reward 56.249959\n",
      "running time 472.677137\n",
      "Train_EnvstepsSoFar : 150001\n",
      "Train_AverageReturn : 51.538692779541904\n",
      "Train_BestReturn : 56.24995859457739\n",
      "TimeSinceStart : 472.6771366596222\n",
      "Training Loss : 0.8921778202056885\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 151000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 152000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 153000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 154000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 155000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 156000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 157000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 158000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 159000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 160000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 160001\n",
      "mean reward (100 episodes) 47.111708\n",
      "best mean reward 56.249959\n",
      "running time 502.316839\n",
      "Train_EnvstepsSoFar : 160001\n",
      "Train_AverageReturn : 47.111708140069425\n",
      "Train_BestReturn : 56.24995859457739\n",
      "TimeSinceStart : 502.31683921813965\n",
      "Training Loss : 1.0429784059524536\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 161000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 162000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 163000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 164000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 165000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 166000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 167000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 168000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 169000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 170000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 170001\n",
      "mean reward (100 episodes) 50.819488\n",
      "best mean reward 56.249959\n",
      "running time 536.174304\n",
      "Train_EnvstepsSoFar : 170001\n",
      "Train_AverageReturn : 50.81948757033902\n",
      "Train_BestReturn : 56.24995859457739\n",
      "TimeSinceStart : 536.174304485321\n",
      "Training Loss : 0.26170945167541504\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 171000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 172000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 173000 ************\n",
      "\n",
      "Training agent...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 174000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 175000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 176000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 177000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 178000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 179000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 180000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 180001\n",
      "mean reward (100 episodes) 59.463186\n",
      "best mean reward 59.463186\n",
      "running time 564.089904\n",
      "Train_EnvstepsSoFar : 180001\n",
      "Train_AverageReturn : 59.46318571135001\n",
      "Train_BestReturn : 59.46318571135001\n",
      "TimeSinceStart : 564.0899040699005\n",
      "Training Loss : 0.23810216784477234\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 181000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 182000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 183000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 184000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 185000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 186000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 187000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 188000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 189000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 190000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 190001\n",
      "mean reward (100 episodes) 70.014945\n",
      "best mean reward 70.014945\n",
      "running time 591.762355\n",
      "Train_EnvstepsSoFar : 190001\n",
      "Train_AverageReturn : 70.01494458754668\n",
      "Train_BestReturn : 70.01494458754668\n",
      "TimeSinceStart : 591.762354850769\n",
      "Training Loss : 0.2708004117012024\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 191000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 192000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 193000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 194000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 195000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 196000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 197000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 198000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 199000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 200000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 200001\n",
      "mean reward (100 episodes) 72.514191\n",
      "best mean reward 72.514191\n",
      "running time 622.412480\n",
      "Train_EnvstepsSoFar : 200001\n",
      "Train_AverageReturn : 72.5141905224154\n",
      "Train_BestReturn : 72.5141905224154\n",
      "TimeSinceStart : 622.4124796390533\n",
      "Training Loss : 1.0699392557144165\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 201000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 202000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 203000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 204000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 205000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 206000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 207000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 208000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 209000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 210000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 210001\n",
      "mean reward (100 episodes) 78.664547\n",
      "best mean reward 78.664547\n",
      "running time 652.396722\n",
      "Train_EnvstepsSoFar : 210001\n",
      "Train_AverageReturn : 78.66454650894246\n",
      "Train_BestReturn : 78.66454650894246\n",
      "TimeSinceStart : 652.3967216014862\n",
      "Training Loss : 0.2871510982513428\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 211000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 212000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 213000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 214000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 215000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 216000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 217000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 218000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 219000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 220000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 220001\n",
      "mean reward (100 episodes) 84.136746\n",
      "best mean reward 84.136746\n",
      "running time 679.919526\n",
      "Train_EnvstepsSoFar : 220001\n",
      "Train_AverageReturn : 84.13674564893057\n",
      "Train_BestReturn : 84.13674564893057\n",
      "TimeSinceStart : 679.9195263385773\n",
      "Training Loss : 0.10978341102600098\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 221000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 222000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 223000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 224000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 225000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 226000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 227000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 228000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 229000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 230000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 230001\n",
      "mean reward (100 episodes) 87.531794\n",
      "best mean reward 87.531794\n",
      "running time 708.369209\n",
      "Train_EnvstepsSoFar : 230001\n",
      "Train_AverageReturn : 87.53179441135585\n",
      "Train_BestReturn : 87.53179441135585\n",
      "TimeSinceStart : 708.3692090511322\n",
      "Training Loss : 0.22907623648643494\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 231000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 232000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 233000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 234000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 235000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 236000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 237000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 238000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 239000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 240000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 240001\n",
      "mean reward (100 episodes) 82.076914\n",
      "best mean reward 87.531794\n",
      "running time 736.532614\n",
      "Train_EnvstepsSoFar : 240001\n",
      "Train_AverageReturn : 82.07691428527284\n",
      "Train_BestReturn : 87.53179441135585\n",
      "TimeSinceStart : 736.5326144695282\n",
      "Training Loss : 1.0426366329193115\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 241000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 242000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 243000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 244000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 245000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 246000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 247000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 248000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 249000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 250000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 250001\n",
      "mean reward (100 episodes) 79.776078\n",
      "best mean reward 87.531794\n",
      "running time 766.015315\n",
      "Train_EnvstepsSoFar : 250001\n",
      "Train_AverageReturn : 79.77607781867191\n",
      "Train_BestReturn : 87.53179441135585\n",
      "TimeSinceStart : 766.0153150558472\n",
      "Training Loss : 0.12873275578022003\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 251000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 252000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 253000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 254000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 255000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 256000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 257000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 258000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 259000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 260000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 260001\n",
      "mean reward (100 episodes) 74.858620\n",
      "best mean reward 87.531794\n",
      "running time 793.744783\n",
      "Train_EnvstepsSoFar : 260001\n",
      "Train_AverageReturn : 74.85861984207294\n",
      "Train_BestReturn : 87.53179441135585\n",
      "TimeSinceStart : 793.7447834014893\n",
      "Training Loss : 0.09465506672859192\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 261000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 262000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 263000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 264000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 265000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 266000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 267000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 268000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 269000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 270000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 270001\n",
      "mean reward (100 episodes) 69.440198\n",
      "best mean reward 87.531794\n",
      "running time 823.158648\n",
      "Train_EnvstepsSoFar : 270001\n",
      "Train_AverageReturn : 69.4401980314906\n",
      "Train_BestReturn : 87.53179441135585\n",
      "TimeSinceStart : 823.15864777565\n",
      "Training Loss : 0.12258537858724594\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 271000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 272000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 273000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 274000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 275000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 276000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 277000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 278000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 279000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 280000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 280001\n",
      "mean reward (100 episodes) 67.772543\n",
      "best mean reward 87.531794\n",
      "running time 850.979194\n",
      "Train_EnvstepsSoFar : 280001\n",
      "Train_AverageReturn : 67.77254342397072\n",
      "Train_BestReturn : 87.53179441135585\n",
      "TimeSinceStart : 850.9791944026947\n",
      "Training Loss : 0.595614492893219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 281000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 282000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 283000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 284000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 285000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 286000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 287000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 288000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 289000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 290000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 290001\n",
      "mean reward (100 episodes) 74.918254\n",
      "best mean reward 87.531794\n",
      "running time 879.387746\n",
      "Train_EnvstepsSoFar : 290001\n",
      "Train_AverageReturn : 74.91825367265506\n",
      "Train_BestReturn : 87.53179441135585\n",
      "TimeSinceStart : 879.3877458572388\n",
      "Training Loss : 0.6341413259506226\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 291000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 292000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 293000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 294000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 295000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 296000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 297000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 298000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 299000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 300000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 300001\n",
      "mean reward (100 episodes) 89.252354\n",
      "best mean reward 89.252354\n",
      "running time 905.684305\n",
      "Train_EnvstepsSoFar : 300001\n",
      "Train_AverageReturn : 89.25235409306353\n",
      "Train_BestReturn : 89.25235409306353\n",
      "TimeSinceStart : 905.6843047142029\n",
      "Training Loss : 0.13999305665493011\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 301000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 302000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 303000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 304000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 305000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 306000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 307000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 308000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 309000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 310000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 310001\n",
      "mean reward (100 episodes) 71.881646\n",
      "best mean reward 89.252354\n",
      "running time 935.829726\n",
      "Train_EnvstepsSoFar : 310001\n",
      "Train_AverageReturn : 71.88164572626894\n",
      "Train_BestReturn : 89.25235409306353\n",
      "TimeSinceStart : 935.8297262191772\n",
      "Training Loss : 0.128824383020401\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 311000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 312000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 313000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 314000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 315000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 316000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 317000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 318000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 319000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 320000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 320001\n",
      "mean reward (100 episodes) 64.373547\n",
      "best mean reward 89.252354\n",
      "running time 963.729503\n",
      "Train_EnvstepsSoFar : 320001\n",
      "Train_AverageReturn : 64.37354659639577\n",
      "Train_BestReturn : 89.25235409306353\n",
      "TimeSinceStart : 963.7295026779175\n",
      "Training Loss : 1.039897084236145\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 321000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 322000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 323000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 324000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 325000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 326000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 327000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 328000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 329000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 330000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 330001\n",
      "mean reward (100 episodes) 35.943320\n",
      "best mean reward 89.252354\n",
      "running time 992.924931\n",
      "Train_EnvstepsSoFar : 330001\n",
      "Train_AverageReturn : 35.94331995808644\n",
      "Train_BestReturn : 89.25235409306353\n",
      "TimeSinceStart : 992.9249310493469\n",
      "Training Loss : 0.40308040380477905\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 331000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 332000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 333000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 334000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 335000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 336000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 337000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 338000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 339000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 340000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 340001\n",
      "mean reward (100 episodes) 40.187862\n",
      "best mean reward 89.252354\n",
      "running time 1018.470356\n",
      "Train_EnvstepsSoFar : 340001\n",
      "Train_AverageReturn : 40.1878619517777\n",
      "Train_BestReturn : 89.25235409306353\n",
      "TimeSinceStart : 1018.4703559875488\n",
      "Training Loss : 0.401751309633255\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 341000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 342000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 343000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 344000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 345000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 346000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 347000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 348000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 349000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 350000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 350001\n",
      "mean reward (100 episodes) 21.296638\n",
      "best mean reward 89.252354\n",
      "running time 1045.449668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_EnvstepsSoFar : 350001\n",
      "Train_AverageReturn : 21.296638442466264\n",
      "Train_BestReturn : 89.25235409306353\n",
      "TimeSinceStart : 1045.4496676921844\n",
      "Training Loss : 0.16915282607078552\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 351000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 352000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 353000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 354000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 355000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 356000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 357000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 358000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 359000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 360000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 360001\n",
      "mean reward (100 episodes) 1.902393\n",
      "best mean reward 89.252354\n",
      "running time 1071.445028\n",
      "Train_EnvstepsSoFar : 360001\n",
      "Train_AverageReturn : 1.902392896051537\n",
      "Train_BestReturn : 89.25235409306353\n",
      "TimeSinceStart : 1071.4450278282166\n",
      "Training Loss : 0.28402113914489746\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 361000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 362000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 363000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 364000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 365000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 366000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 367000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 368000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 369000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 370000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 370001\n",
      "mean reward (100 episodes) 15.575356\n",
      "best mean reward 89.252354\n",
      "running time 1100.888237\n",
      "Train_EnvstepsSoFar : 370001\n",
      "Train_AverageReturn : 15.575355751675467\n",
      "Train_BestReturn : 89.25235409306353\n",
      "TimeSinceStart : 1100.8882369995117\n",
      "Training Loss : 0.3198748826980591\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 371000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 372000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 373000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 374000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 375000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 376000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 377000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 378000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 379000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 380000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 380001\n",
      "mean reward (100 episodes) 32.322743\n",
      "best mean reward 89.252354\n",
      "running time 1127.908297\n",
      "Train_EnvstepsSoFar : 380001\n",
      "Train_AverageReturn : 32.32274251120962\n",
      "Train_BestReturn : 89.25235409306353\n",
      "TimeSinceStart : 1127.9082973003387\n",
      "Training Loss : 0.7184828519821167\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 381000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 382000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 383000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 384000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 385000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 386000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 387000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 388000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 389000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 390000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 390001\n",
      "mean reward (100 episodes) 58.095419\n",
      "best mean reward 89.252354\n",
      "running time 1155.204987\n",
      "Train_EnvstepsSoFar : 390001\n",
      "Train_AverageReturn : 58.095419328348434\n",
      "Train_BestReturn : 89.25235409306353\n",
      "TimeSinceStart : 1155.2049865722656\n",
      "Training Loss : 7.069092273712158\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 391000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 392000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 393000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 394000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 395000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 396000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 397000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 398000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 399000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 400000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 400001\n",
      "mean reward (100 episodes) 85.666074\n",
      "best mean reward 89.252354\n",
      "running time 1181.423796\n",
      "Train_EnvstepsSoFar : 400001\n",
      "Train_AverageReturn : 85.66607385571334\n",
      "Train_BestReturn : 89.25235409306353\n",
      "TimeSinceStart : 1181.423796415329\n",
      "Training Loss : 0.8080357313156128\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 401000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 402000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 403000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 404000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 405000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 406000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 407000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 408000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 409000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 410000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 410001\n",
      "mean reward (100 episodes) 95.181187\n",
      "best mean reward 95.181187\n",
      "running time 1207.087339\n",
      "Train_EnvstepsSoFar : 410001\n",
      "Train_AverageReturn : 95.18118725839493\n",
      "Train_BestReturn : 95.18118725839493\n",
      "TimeSinceStart : 1207.0873391628265\n",
      "Training Loss : 1.1735098361968994\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 411000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 412000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 413000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 414000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 415000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 416000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 417000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 418000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 419000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 420000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 420001\n",
      "mean reward (100 episodes) 108.076698\n",
      "best mean reward 108.076698\n",
      "running time 1232.669085\n",
      "Train_EnvstepsSoFar : 420001\n",
      "Train_AverageReturn : 108.07669766345708\n",
      "Train_BestReturn : 108.07669766345708\n",
      "TimeSinceStart : 1232.6690850257874\n",
      "Training Loss : 0.18103322386741638\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 421000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 422000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 423000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 424000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 425000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 426000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 427000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 428000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 429000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 430000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 430001\n",
      "mean reward (100 episodes) 101.689028\n",
      "best mean reward 108.076698\n",
      "running time 1262.071038\n",
      "Train_EnvstepsSoFar : 430001\n",
      "Train_AverageReturn : 101.68902750938514\n",
      "Train_BestReturn : 108.07669766345708\n",
      "TimeSinceStart : 1262.0710384845734\n",
      "Training Loss : 1.1306705474853516\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 431000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 432000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 433000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 434000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 435000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 436000 ************\n",
      "\n",
      "Training agent...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Iteration 437000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 438000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 439000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 440000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 440001\n",
      "mean reward (100 episodes) 96.152777\n",
      "best mean reward 108.076698\n",
      "running time 1291.139858\n",
      "Train_EnvstepsSoFar : 440001\n",
      "Train_AverageReturn : 96.15277723494162\n",
      "Train_BestReturn : 108.07669766345708\n",
      "TimeSinceStart : 1291.139858007431\n",
      "Training Loss : 0.271650493144989\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 441000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 442000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 443000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 444000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 445000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 446000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 447000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 448000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 449000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 450000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 450001\n",
      "mean reward (100 episodes) 83.658144\n",
      "best mean reward 108.076698\n",
      "running time 1316.279222\n",
      "Train_EnvstepsSoFar : 450001\n",
      "Train_AverageReturn : 83.65814364824611\n",
      "Train_BestReturn : 108.07669766345708\n",
      "TimeSinceStart : 1316.2792220115662\n",
      "Training Loss : 0.13873451948165894\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 451000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 452000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 453000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 454000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 455000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 456000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 457000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 458000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 459000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 460000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 460001\n",
      "mean reward (100 episodes) 62.123148\n",
      "best mean reward 108.076698\n",
      "running time 1346.851308\n",
      "Train_EnvstepsSoFar : 460001\n",
      "Train_AverageReturn : 62.12314793612093\n",
      "Train_BestReturn : 108.07669766345708\n",
      "TimeSinceStart : 1346.851308107376\n",
      "Training Loss : 0.5410489439964294\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 461000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 462000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 463000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 464000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 465000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 466000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 467000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 468000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 469000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 470000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 470001\n",
      "mean reward (100 episodes) 48.584682\n",
      "best mean reward 108.076698\n",
      "running time 1383.545558\n",
      "Train_EnvstepsSoFar : 470001\n",
      "Train_AverageReturn : 48.58468182896072\n",
      "Train_BestReturn : 108.07669766345708\n",
      "TimeSinceStart : 1383.5455584526062\n",
      "Training Loss : 1.3411774635314941\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 471000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 472000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 473000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 474000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 475000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 476000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 477000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 478000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 479000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 480000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 480001\n",
      "mean reward (100 episodes) 55.288593\n",
      "best mean reward 108.076698\n",
      "running time 1421.427022\n",
      "Train_EnvstepsSoFar : 480001\n",
      "Train_AverageReturn : 55.28859307695755\n",
      "Train_BestReturn : 108.07669766345708\n",
      "TimeSinceStart : 1421.4270215034485\n",
      "Training Loss : 0.19222936034202576\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 481000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 482000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 483000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 484000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 485000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 486000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 487000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 488000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 489000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 490000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 490001\n",
      "mean reward (100 episodes) 54.984221\n",
      "best mean reward 108.076698\n",
      "running time 1459.803380\n",
      "Train_EnvstepsSoFar : 490001\n",
      "Train_AverageReturn : 54.98422124702376\n",
      "Train_BestReturn : 108.07669766345708\n",
      "TimeSinceStart : 1459.8033804893494\n",
      "Training Loss : 0.13950490951538086\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 491000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 492000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 493000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 494000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 495000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 496000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 497000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 498000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 499000 ************\n",
      "\n",
      "Training agent...\n"
     ]
    }
   ],
   "source": [
    "#@title run training\n",
    "\n",
    "trainer = Q_Trainer(args)\n",
    "trainer.run_training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_kTH-tXkI-B-"
   },
   "outputs": [],
   "source": [
    "#@markdown You can visualize your runs with tensorboard from within the notebook\n",
    "\n",
    "## requires tensorflow==2.3.0\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /content/cs285_f2020/homework_fall2020/hw3/data/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "run_hw3_dqn.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "sds",
   "language": "python",
   "name": "sds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
