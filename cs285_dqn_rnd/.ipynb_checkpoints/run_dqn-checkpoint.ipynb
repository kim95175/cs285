{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7fca57",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt update \n",
    "!apt install -y --no-install-recommends \\\n",
    "        build-essential \\\n",
    "        curl \\\n",
    "        git \\\n",
    "        gnupg2 \\\n",
    "        make \\\n",
    "        cmake \\\n",
    "        ffmpeg \\\n",
    "        swig \\\n",
    "        libz-dev \\\n",
    "        unzip \\\n",
    "        zlib1g-dev \\\n",
    "        libglfw3 \\\n",
    "        libglfw3-dev \\\n",
    "        libxrandr2 \\\n",
    "        libxinerama-dev \\\n",
    "        libxi6 \\\n",
    "        libxcursor-dev \\\n",
    "        libgl1-mesa-dev \\\n",
    "        libgl1-mesa-glx \\\n",
    "        libglew-dev \\\n",
    "        libosmesa6-dev \\\n",
    "        lsb-release \\\n",
    "        ack-grep \\\n",
    "        patchelf \\\n",
    "        wget \\\n",
    "        xpra \\\n",
    "        xserver-xorg-dev \\\n",
    "        xvfb \\\n",
    "        python-opengl \\\n",
    "        ffmpeg > /dev/null 2>&1\n",
    "\n",
    "!pip install opencv-python==3.4.0.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2a262a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfab88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "# For later\n",
    "from colab_utils import (\n",
    "    wrap_env,\n",
    "    show_video\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4d802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from cs285.infrastructure.rl_trainer import RL_Trainer\n",
    "from cs285.agents.dqn_agent import DQNAgent\n",
    "from cs285.infrastructure.dqn_utils import get_env_kwargs\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdeae477",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "\n",
    "  def __getitem__(self, key):\n",
    "    return getattr(self, key)\n",
    "\n",
    "  def __setitem__(self, key, val):\n",
    "    setattr(self, key, val)\n",
    "\n",
    "  def __contains__(self, key):\n",
    "    return hasattr(self, key)\n",
    "\n",
    "  env_name = 'LunarLander-v3' #@param ['MsPacman-v0', 'LunarLander-v3', 'PongNoFrameSkip-v4']\n",
    "  exp_name = 'q3_dqn' #@param\n",
    "\n",
    "  ## PDF will tell you how to set ep_len\n",
    "  ## and discount for each environment\n",
    "  ep_len = 200 #@param {type: \"integer\"}\n",
    "\n",
    "  #@markdown batches and steps\n",
    "  batch_size = 32 #@param {type: \"integer\"}\n",
    "  eval_batch_size = 1000 #@param {type: \"integer\"}\n",
    "\n",
    "  num_agent_train_steps_per_iter = 1 #@param {type: \"integer\"}\n",
    "\n",
    "  num_critic_updates_per_agent_update = 1 #@param {type: \"integer\"}\n",
    "  \n",
    "  #@markdown Q-learning parameters\n",
    "  double_q = False #@param {type: \"boolean\"}\n",
    "\n",
    "  #@markdown system\n",
    "  save_params = False #@param {type: \"boolean\"}\n",
    "  no_gpu = False #@param {type: \"boolean\"}\n",
    "  which_gpu = 0 #@param {type: \"integer\"}\n",
    "  seed = 1 #@param {type: \"integer\"}\n",
    "\n",
    "  #@markdown logging\n",
    "  ## default is to not log video so\n",
    "  ## that logs are small enough to be\n",
    "  ## uploaded to gradscope\n",
    "  video_log_freq =  -1 #@param {type: \"integer\"}\n",
    "  scalar_log_freq =  10000#@param {type: \"integer\"}\n",
    "\n",
    "\n",
    "args = Args()\n",
    "\n",
    "## ensure compatibility with hw1 code\n",
    "args['train_batch_size'] = args['batch_size']\n",
    "\n",
    "if args['video_log_freq'] > 0:\n",
    "  import warnings\n",
    "  warnings.warn(\n",
    "      '''\\nLogging videos will make eventfiles too'''\n",
    "      '''\\nlarge for the autograder. Set video_log_freq = -1'''\n",
    "      '''\\nfor the runs you intend to submit.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd105c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title create directories for logging\n",
    "\n",
    "data_path = '''/content/cs285_f2020/''' \\\n",
    "        '''homework_fall2020/hw3/data'''\n",
    "\n",
    "if not (os.path.exists(data_path)):\n",
    "    os.makedirs(data_path)\n",
    "\n",
    "logdir = 'hw3_' + args.exp_name + '_' + args.env_name + '_' + time.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "logdir = os.path.join(data_path, logdir)\n",
    "args['logdir'] = logdir\n",
    "if not(os.path.exists(logdir)):\n",
    "    os.makedirs(logdir)\n",
    "\n",
    "print(\"LOGGING TO: \", logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3928f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define Q-function trainer\n",
    "\n",
    "class Q_Trainer(object):\n",
    "\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "\n",
    "        train_args = {\n",
    "            'num_agent_train_steps_per_iter': params['num_agent_train_steps_per_iter'],\n",
    "            'num_critic_updates_per_agent_update': params['num_critic_updates_per_agent_update'],\n",
    "            'train_batch_size': params['batch_size'],\n",
    "            'double_q': params['double_q'],\n",
    "        }\n",
    "\n",
    "        env_args = get_env_kwargs(params['env_name'])\n",
    "\n",
    "        for k, v in env_args.items():\n",
    "          params[k] = v\n",
    "\n",
    "        self.params['agent_class'] = DQNAgent\n",
    "        self.params['agent_params'] = params\n",
    "        self.params['train_batch_size'] = params['batch_size']\n",
    "        self.params['env_wrappers'] = env_args['env_wrappers']\n",
    "\n",
    "        self.rl_trainer = RL_Trainer(self.params)\n",
    "\n",
    "    def run_training_loop(self):\n",
    "        self.rl_trainer.run_training_loop(\n",
    "            self.params['num_timesteps'],\n",
    "            collect_policy = self.rl_trainer.agent.actor,\n",
    "            eval_policy = self.rl_trainer.agent.actor,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88e73af",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Q_Trainer(args)\n",
    "trainer.run_training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3601462",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown You can visualize your runs with tensorboard from within the notebook\n",
    "\n",
    "## requires tensorflow==2.3.0\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir data/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:elsa] *",
   "language": "python",
   "name": "conda-env-elsa-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
